{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping # type: ignore\n",
    "import keras.backend as K # type: ignore\n",
    "from keras.models import Sequential # type: ignore\n",
    "from keras.layers import LSTM,Dense, Bidirectional, GRU, Dropout, BatchNormalization, SimpleRNN # type: ignore\n",
    "from keras.regularizers import l2 # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "import longitudinal_tadpole.pipeline as p\n",
    "from longitudinal_tadpole.threshmax import ThreshMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/tadpole_data/tadpole_data_time_imptn_norm_thrshld30.pickle\", \"rb\") as file:\n",
    "    data_nested_dict = pkl.load(file)\n",
    "with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/tadpole_data/tadpole_labels_time_imptn_norm_thrshld30.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bps = []\n",
    "for i in range(5):\n",
    "    with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/base_predictions/multiclass/data_at_n_w_labels_at_n/no_sampling/split_{i}.pkl\", \"rb\") as file:\n",
    "        split = pkl.load(file)\n",
    "        bps.append(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y):\n",
    "    labels_across_time = np.eye(3)[y]\n",
    "\n",
    "    return labels_across_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights_for_t(training_labels):\n",
    "    class_weights_for_labels = []\n",
    "    for t in range(training_labels.shape[1]):\n",
    "        bin_counts_per_t = 1 / np.bincount(training_labels[:,t])\n",
    "        norm_bin_counts_per_t = bin_counts_per_t/np.sum(bin_counts_per_t)\n",
    "        class_weights_t = dict(zip(range(3), norm_bin_counts_per_t))\n",
    "        class_weights_for_labels.append(class_weights_t)\n",
    "    return class_weights_for_labels\n",
    "    # mapped_array = [np.array([class_weights_for_labels[t][entry] for entry in training_labels[:,t]]) for t in range(training_labels.shape[1])]\n",
    "    # mapped_array = np.stack(mapped_array, axis=1)\n",
    "    # return tf.constant(mapped_array, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal categorical crossentropy. Weighs error by how many classes output was off by. weights range from 1 to 2. assigns class weights across time.\n",
    "# https://stats.stackexchange.com/questions/87826/machine-learning-with-ordered-labels\n",
    "#gamma = d(MCI,CN) - d(MCI,Dementia) = d(MCI,CN) - 1\n",
    "\n",
    "def occ_loss(gamma=0):\n",
    "    global class_weights\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_ord = tf.argmax(y_true, axis=-1)\n",
    "        y_pred_ord = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "        losses_over_time = []\n",
    "        for t in range(y_true.shape[1]):\n",
    "            y_true_ord_t = y_true_ord[:,t]\n",
    "            \n",
    "            class_weights_t = class_weights[t]\n",
    "            w_c_t = tf.gather(tf.constant(list(class_weights_t.values()), dtype=tf.float32),\n",
    "                                y_true_ord_t)\n",
    "\n",
    "            if gamma=='none':\n",
    "                loss = tf.keras.losses.categorical_crossentropy(y_true[:,t], y_pred[:,t]) * w_c_t\n",
    "            else:\n",
    "                y_true_ord_gamma = y_true_ord_t + tf.cast(y_true_ord_t != 0, tf.int64) * gamma\n",
    "                y_pred_ord_gamma = y_pred_ord[:,t] + tf.cast(y_pred_ord[:,t] != 0, tf.int64) * gamma\n",
    "                w_o_t = tf.cast(tf.abs(y_true_ord_gamma - y_pred_ord_gamma) / (2 + gamma), dtype='float32') + 1\n",
    "    \n",
    "                loss = tf.keras.losses.categorical_crossentropy(y_true[:,t], y_pred[:,t]) * w_o_t * w_c_t\n",
    "\n",
    "            losses_over_time.append(loss)\n",
    "        return tf.reduce_mean(tf.stack(losses_over_time, axis=-1), axis=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dem_prev(label_seq):\n",
    "    return [np.sum(y==2) for y in label_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data(dfs):\n",
    "    #reformat labels\n",
    "    labels_across_time = np.column_stack([df['labels'].values for df in dfs])\n",
    "    labels_across_time = np.eye(3)[labels_across_time]\n",
    "    \n",
    "    # reformat data\n",
    "    RNN_training_data_fold = [df.drop(columns=[\"labels\"], axis=1, level=0) for df in dfs]\n",
    "    data_arrays_per_timepoint = [df.to_numpy() for df in RNN_training_data_fold]\n",
    "    tensor_3d = np.stack(data_arrays_per_timepoint, axis=1)\n",
    "\n",
    "    return tensor_3d, labels_across_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_restandardize(data):\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    \n",
    "    #for restandardizing bl class 2 columns to be all -1\n",
    "    means[means==0] = 1\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    restandardized_data = (data-means)/stds\n",
    "\n",
    "    return restandardized_data, means, stds\n",
    "\n",
    "def restandardize(data, means, stds):\n",
    "\n",
    "    return (data-means)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train lstms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use data up to time point n, \n",
    "#e.g., when n=3 means use data from t=0,1,2 and predict on label from t=3\n",
    "cell_list = [\"LSTM\"]##, \"GRU\"]#, 'RNN', \"biLSTM\", \"biGRU\"]\n",
    "def train_eval_RNNs(train, test, seed, cells=cell_list, decision='argmax', random_bl=False, sampling=None, gamma=0):\n",
    "    global f_scores_test\n",
    "    global p_scores_test\n",
    "    global r_scores_test\n",
    "    # global f_scores_train\n",
    "    for cell in cells:\n",
    "        y_preds_test = []\n",
    "        y_tests = []\n",
    "        \n",
    "        y_preds_train = []\n",
    "        y_trains = []\n",
    "\n",
    "        y_preds_val = []\n",
    "        y_vals = []\n",
    "        \n",
    "        for fold, dfs in enumerate(train):\n",
    "            if random_bl==False:\n",
    "                real=\"Real\"\n",
    "            else:\n",
    "                real=\"Random\"\n",
    "            print(real, cell, \"seed\",seed+1, \"fold\", fold+1, \"gamma=\",gamma)\n",
    "            \n",
    "            X_train, y_train = reformat_data(dfs)\n",
    "            X_test, y_test = reformat_data(test[fold])\n",
    "\n",
    "\n",
    "            X_train, X_test = X_train[:, :-1, :], X_test[:, :-1, :] #data excluding last time point\n",
    "            y_train, y_test = y_train[:, 1:, :], y_test[:, 1:, :] #labels excluding baseline\n",
    "\n",
    "\n",
    "            #restandardize to have mean=0, std=1 (for tanh)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.09,\n",
    "                                                                stratify=dem_prev(y_train), random_state=seed**2)\n",
    "            X_train, means, stds = fit_restandardize(X_train)\n",
    "            X_test, X_val = restandardize(X_test, means=means, stds=stds), restandardize(X_val, means=means, stds=stds)\n",
    "            \n",
    "\n",
    "            \n",
    "            y_trains.append(y_train)\n",
    "            y_tests.append(y_test)\n",
    "            y_vals.append(y_val)\n",
    "\n",
    "            \n",
    "            y_train_ord = np.argmax(y_train, axis=-1)\n",
    "            global class_weights\n",
    "            class_weights = [dict(zip(range(3), compute_class_weight(class_weight='balanced', classes=[0,1,2], y=y_train_ord[:,t]))) for t in range(y_train_ord.shape[1])]\n",
    "            \n",
    "            model = p.build_RNN(units=64, cell=cell, drout=0.2, deep=True, L2=0.00,\n",
    "                               activation='tanh', reg_layer='batchnorm', gamma=gamma)\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss', patience=30, verbose=1,\n",
    "                restore_best_weights=True, start_from_epoch=10)\n",
    "            \n",
    "            \n",
    "            if random_bl:\n",
    "                np.random.shuffle(y_train)\n",
    "                np.random.shuffle(y_val)\n",
    "\n",
    "            #fit model\n",
    "            model.fit(X_train, y_train, epochs=2000, validation_data=[X_val, y_val],\n",
    "                       verbose=1, callbacks=[early_stop])\n",
    "\n",
    "            y_preds_train.append(model.predict(X_train))\n",
    "            y_preds_test.append(model.predict(X_test))\n",
    "            y_preds_val.append(model.predict(X_val))\n",
    "        \n",
    "        y_pred_train = np.concatenate(y_preds_train, axis=0)\n",
    "        y_train = np.concatenate(y_trains, axis=0)\n",
    "\n",
    "        y_pred_test = np.concatenate(y_preds_test, axis=0)\n",
    "        y_test = np.concatenate(y_tests, axis=0)\n",
    "       \n",
    "        y_pred_val = np.concatenate(y_preds_val, axis=0)\n",
    "        y_val = np.concatenate(y_vals, axis=0)\n",
    "\n",
    "        if decision == \"argmax\": # for doing traditional decision making\n",
    "            y_test_ord = np.argmax(y_test, axis=-1)\n",
    "            f_scores_cell_seed_across_time_test = np.stack([f1_score(y_test_ord[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            p_scores_cell_seed_across_time_test = np.stack([precision_score(y_test_ord[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            r_scores_cell_seed_across_time_test = np.stack([recall_score(y_test_ord[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "\n",
    "\n",
    "        elif decision == 'threshmax':\n",
    "            y_train_ord = np.argmax(y_train, axis=-1)\n",
    "            y_test_ord = np.argmax(y_test, axis=-1)\n",
    "            y_val_ord = np.argmax(y_val, axis=-1)\n",
    "            \n",
    "            y_not_test_ord = np.concatenate([y_train_ord, y_val_ord], axis=0)\n",
    "            y_pred_not_test = np.concatenate([y_pred_train, y_pred_val], axis=0)\n",
    "\n",
    "            tm = ThreshMax(classes=[0,1,2], thresh_class=1, class_to_optimize='avg')\n",
    "\n",
    "            thresholds = [tm.find_tmax(y_trues=y_not_test_ord[:,i], y_preds=y_pred_not_test[:,i,:]) for i in range(y_not_test_ord.shape[1])]\n",
    "            f_scores_cell_seed_across_time_test = np.stack([f1_score(y_test_ord[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            p_scores_cell_seed_across_time_test = np.stack([precision_score(y_test_ord[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            r_scores_cell_seed_across_time_test = np.stack([recall_score(y_test_ord[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "        \n",
    "        f_scores_test[f'{real} {cell}'].append(f_scores_cell_seed_across_time_test)\n",
    "        p_scores_test[f'{real} {cell}'].append(p_scores_cell_seed_across_time_test)\n",
    "        r_scores_test[f'{real} {cell}'].append(r_scores_cell_seed_across_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores_test = dict()\n",
    "p_scores_test = dict()\n",
    "r_scores_test = dict()\n",
    "y_test_predictions = dict()\n",
    "for cell in cell_list:\n",
    "    for real in ['Real']:#, 'Random']:\n",
    "        f_scores_test[f'{real} {cell}'] = []\n",
    "        p_scores_test[f'{real} {cell}'] = []\n",
    "        r_scores_test[f'{real} {cell}'] = []\n",
    "        y_test_predictions[f'{real} {cell}'] = []\n",
    "\n",
    "# f_scores_train = {k: [] for k in cell_list} #gets score distribution for diff cells across splits\n",
    "for seed in range(5): #range(len(bps)):\n",
    "    for gamma in range(5):\n",
    "        train_eval_RNNs(train=bps[seed][0], test=bps[seed][1], seed=seed, decision='threshmax', random_bl=False, sampling=None, gamma=gamma)\n",
    "    # train_eval_RNNs(train=bps[seed][0], test=bps[seed][1], seed=seed, decision='argmax', random_bl=True, sampling='dem_prev')\n",
    "\n",
    "f_score_arrays_test = {k: np.stack(v, axis=0) for k,v in f_scores_test.items()}\n",
    "p_score_arrays_test = {k: np.stack(v, axis=0) for k,v in p_scores_test.items()}\n",
    "r_score_arrays_test = {k: np.stack(v, axis=0) for k,v in r_scores_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(f_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}\n",
    "p_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(p_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}\n",
    "r_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(r_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}\n",
    "\n",
    "for seed in range(5): #range(len(bps)):\n",
    "    train_eval_RNNs(train=bps[seed][0], test=bps[seed][1], seed=seed, decision='threshmax', random_bl=False, sampling=None, gamma='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores_gamma['no gamma'] = np.stack(f_scores_test['Real LSTM'][-5:], axis=0)\n",
    "p_scores_gamma['no gamma'] = np.stack(p_scores_test['Real LSTM'][-5:], axis=0)\n",
    "r_scores_gamma['no gamma'] = np.stack(r_scores_test['Real LSTM'][-5:], axis=0)\n",
    "gamma_results = {'f': f_scores_gamma, 'p': p_scores_gamma, 'r': r_scores_gamma}    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
