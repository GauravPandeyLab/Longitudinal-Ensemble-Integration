{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from eipy.metrics import fmax_score\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eipy.ei as e\n",
    "import os\n",
    "import pickle as pkl\n",
    "import longitudinal_tadpole.pipeline as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "            'f_1': f1_score,\n",
    "            'auc': roc_auc_score,\n",
    "            'mcc': matthews_corrcoef\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictors = {\n",
    "                    'ADAB': AdaBoostClassifier(),\n",
    "                    'XGB': XGBClassifier(),\n",
    "                    'DT': DecisionTreeClassifier(),\n",
    "                    'RF': RandomForestClassifier(),\n",
    "                    'GB': GradientBoostingClassifier(),\n",
    "                    'KNN': KNeighborsClassifier(),\n",
    "                    'LR': LogisticRegression(),\n",
    "                    'NB': GaussianNB(),\n",
    "                    'MLP': MLPClassifier(),\n",
    "                    'SVM': SVC(probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "with open(f\"{cwd}/longitudinal_tadpole/tadpole_data/tadpole_data_as_dfs/tadpole_data_time_imptn_norm_thrshld30_dfs.pickle\", \"rb\") as file:\n",
    "    data_nested_dict = pkl.load(file)\n",
    "with open(f\"{cwd}/longitudinal_tadpole/tadpole_data/tadpole_data_as_dfs/tadpole_labels_time_imptn_norm_thrshld30_dfs.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_of_dicts = [data_nested_dict[k] for k in data_nested_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in labels.items():\n",
    "    labels[k] = v.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate transformation to make sure labels are ordered correctly in time\n",
    "labels = pd.DataFrame(labels)\n",
    "\n",
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for multiclass version of data\n",
    "encoding_dict = {'NL': 0, 'MCI': 1, 'Dementia': 2}\n",
    "\n",
    "labels = np.vectorize(lambda x: encoding_dict[x])(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate bp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(df):\n",
    "    column_names = []\n",
    "    for i in range(df.columns.nlevels):\n",
    "        if i == 0:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(\"labels\"))\n",
    "            \n",
    "        else:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(''))\n",
    "    \n",
    "    return column_names\n",
    "\n",
    "def fix_first_time_point(df):\n",
    "    new_columns = get_column_names(df)\n",
    "    classes=[0,1,2]\n",
    "    new_columns.append(classes)\n",
    "    new_mux=pd.MultiIndex.from_product(iterables=new_columns, names=[\"modality\", \"base predictor\", \"sample\", \"class\"])\n",
    "    new_df = pd.DataFrame(columns=new_mux)\n",
    "\n",
    "    for col in new_df.columns:\n",
    "        if col[-1] == 0:\n",
    "            new_df[col] = 1 - df[col[:-1]]\n",
    "        elif col[-1] == 1:\n",
    "            new_df[col] = df[col[:-1]]\n",
    "        else:\n",
    "            new_df[col] = 0\n",
    "    \n",
    "    new_df['labels'] = df['labels']\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_RNN_data(seed, aligned=True):\n",
    "    meta_data = []\n",
    "    for t in range(len(data_list_of_dicts)):\n",
    "        #time dependent data splitting\n",
    "        X_train_test_timestep = data_list_of_dicts[t]\n",
    "        #labels_at_timestep = labels[:,t] #for no sampling\n",
    "        EI_for_timestep = e.EnsembleIntegration(\n",
    "                            base_predictors=base_predictors,\n",
    "                            k_outer=5,\n",
    "                            k_inner=5,\n",
    "                            n_samples=1,\n",
    "                            sampling_strategy='oversampling',\n",
    "                            n_jobs=-1,\n",
    "                            metrics=metrics,\n",
    "                            random_state=seed,\n",
    "                            project_name=f'time step {t}',\n",
    "                            model_building=True,\n",
    "                            time_series= (1,t)\n",
    "                            )\n",
    "        print(f\"generating metadata for timestep {t}\")\n",
    "        EI_for_timestep.fit_base(X_train_test_timestep, labels) #y=labels_at_timestep\n",
    "        meta_data.append([EI_for_timestep.ensemble_training_data, EI_for_timestep.ensemble_test_data, EI_for_timestep.base_summary, EI_for_timestep])\n",
    "\n",
    "    #swap arrangement across folds and time\n",
    "    RNN_training_data = [[dfs[0][i] for dfs in meta_data] for i in range(5)]\n",
    "    RNN_test_data = [[dfs[1][i] for dfs in meta_data] for i in range(5)]\n",
    "    base_summaries = [x[-2] for x in meta_data]\n",
    "    EIs = [x[-1] for x in meta_data]\n",
    "\n",
    "    for i in range(len(RNN_training_data)):\n",
    "        RNN_training_data[i][0] = fix_first_time_point(RNN_training_data[i][0])\n",
    "        RNN_test_data[i][0] = fix_first_time_point(RNN_test_data[i][0])\n",
    "    \n",
    "    return RNN_training_data, RNN_test_data, base_summaries, EIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data at final timepoint is copy of data from 2nd to last timepoint. Only there to make downstream formatting easier.\n",
    "for seed in range(5):\n",
    "    with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/base_predictions/multiclass/data_at_n_w_labels_at_n/oversampling/split_{seed}.pkl\", \"wb\") as file:\n",
    "        base_prediction_data = generate_RNN_data(seed=seed, aligned=True)\n",
    "        pkl.dump(obj=base_prediction_data, file=file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
