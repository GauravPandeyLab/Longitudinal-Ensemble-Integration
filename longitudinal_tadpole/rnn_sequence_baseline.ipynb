{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eipy.ei as e\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, Callback # type: ignore\n",
    "from tensorboard import notebook\n",
    "import keras.backend as K # type: ignore\n",
    "from keras.models import Sequential, Model # type: ignore\n",
    "from keras.layers import LSTM,Dense, Bidirectional, GRU, Dropout # type: ignore\n",
    "from keras.layers import BatchNormalization, SimpleRNN, Input, Lambda, TimeDistributed # type: ignore\n",
    "from keras.regularizers import l2 # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "import longitudinal_tadpole.pipeline as p\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from longitudinal_tadpole.threshmax import ThreshMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/tadpole_data/tadpole_data_time_imptn_norm_thrshld30.pickle\", \"rb\") as file:\n",
    "    data_nested_dict = pkl.load(file)\n",
    "with open(f\"/Users/susmaa01/Documents/eipy/longitudinal_tadpole/tadpole_data/tadpole_labels_time_imptn_norm_thrshld30.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = {}\n",
    "for k,v in data_nested_dict.items():\n",
    "    list_of_data_arrays = [data_array for data_array in v.values()]\n",
    "    concatenated_data[k] = np.concatenate(list_of_data_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([v for v in concatenated_data.values()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, y = p.initial_data_label_prep(data_nested_dict, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y):\n",
    "    labels_across_time = np.eye(3)[y]\n",
    "\n",
    "    return labels_across_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_offset = y[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal categorical crossentropy. Weighs error by how many classes output was off by. weights range from 1 to 2. assigns class weights across time.\n",
    "# https://stats.stackexchange.com/questions/87826/machine-learning-with-ordered-labels\n",
    "#gamma = d(MCI,CN) - d(MCI,Dementia) = d(MCI,CN) - 1\n",
    "\n",
    "def occ_loss(gamma=0):\n",
    "    global class_weights\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_ord = tf.argmax(y_true, axis=-1)\n",
    "        y_pred_ord = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "        losses_over_time = []\n",
    "        for t in range(4):\n",
    "            y_true_ord_t = y_true_ord[:,t]\n",
    "            \n",
    "            class_weights_t = class_weights[t]\n",
    "            w_c_t = tf.gather(tf.constant(list(class_weights_t.values()), dtype=tf.float32),\n",
    "                                y_true_ord_t)\n",
    "\n",
    "            if gamma=='none':\n",
    "                loss = tf.keras.losses.categorical_crossentropy(y_true[:,t], y_pred[:,t]) * w_c_t\n",
    "            else:\n",
    "                y_true_ord_gamma = y_true_ord_t + tf.cast(y_true_ord_t != 0, tf.int64) * gamma\n",
    "                y_pred_ord_gamma = y_pred_ord[:,t] + tf.cast(y_pred_ord[:,t] != 0, tf.int64) * gamma\n",
    "                w_o_t = tf.cast(tf.abs(y_true_ord_gamma - y_pred_ord_gamma) / (2 + gamma), dtype='float32') + 1\n",
    "    \n",
    "                loss = tf.keras.losses.categorical_crossentropy(y_true[:,t], y_pred[:,t]) * w_o_t * w_c_t\n",
    "\n",
    "            losses_over_time.append(loss)\n",
    "        return tf.reduce_mean(tf.stack(losses_over_time, axis=-1), axis=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dem_prev(label_seq):\n",
    "    return [np.sum(y==2) for y in label_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_restandardize(data):\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    \n",
    "    #for restandardizing bl class 2 columns to be all -1\n",
    "    means[means==0] = 1\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    restandardized_data = (data-means)/stds\n",
    "\n",
    "    return restandardized_data, means, stds\n",
    "\n",
    "def restandardize(data, means, stds):\n",
    "\n",
    "    return (data-means)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train lstms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list = [\"LSTM\"]#, \"GRU\"]#, 'RNN', \"biLSTM\", \"biGRU\"]\n",
    "def train_eval_RNNs(seed, decision, cells=cell_list, random_bl=False, sampling=None, multiclass_weights='longitudinal', gamma=0):\n",
    "    global f_scores_test\n",
    "    global p_scores_test\n",
    "    global r_scores_test\n",
    "    for cell in cells:\n",
    "        y_preds_test = []\n",
    "        y_tests = []\n",
    "        \n",
    "        y_preds_train = []\n",
    "        y_trains = []\n",
    "        \n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(X, dem_prev(y_offset)), 1):\n",
    "            if random_bl==False:\n",
    "                real=\"Real\"\n",
    "            else:\n",
    "                real=\"Random\"\n",
    "            print(real, cell, \"seed\",seed+1, \"fold\", fold, \"gamma=\", gamma)\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y_offset[train_index], y_offset[test_index]\n",
    "\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.09,\n",
    "                                                                stratify=dem_prev(y_train), random_state=seed**2)\n",
    "            \n",
    "\n",
    "            y_train_ohe, y_val_ohe = ohe(y_train), ohe(y_val)\n",
    "            y_tests.append(y_test)\n",
    "            y_trains.append(y_train)\n",
    "            \n",
    "\n",
    "            X_train, means, stds = fit_restandardize(X_train)\n",
    "            X_test, X_val = restandardize(X_test, means=means, stds=stds), restandardize(X_val, means=means, stds=stds)\n",
    "\n",
    "            global class_weights\n",
    "            class_weights = [dict(zip(range(3), compute_class_weight(class_weight='balanced', classes=[0,1,2], y=y_train[:,t]))) for t in range(y_train.shape[1])]\n",
    "            \n",
    "\n",
    "            model = p.build_RNN(units=64, cell=cell, drout=0.2, deep=True, L2=0.00,\n",
    "                               activation='tanh', reg_layer='batchnorm', gamma=gamma)\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss', patience=30, verbose=1,\n",
    "                restore_best_weights=True, start_from_epoch=10)\n",
    "            \n",
    "            \n",
    "            if random_bl:\n",
    "                np.random.shuffle(y_train_ohe)\n",
    "                np.random.shuffle(y_val_ohe)\n",
    "\n",
    "            #fit model\n",
    "            model.fit(X_train, y_train_ohe, epochs=2000, validation_data=[X_val, y_val_ohe],\n",
    "                       verbose=1, callbacks=[early_stop])\n",
    "\n",
    "            y_preds_train.append(model.predict(X_train))\n",
    "            y_preds_test.append(model.predict(X_test))\n",
    "        \n",
    "        y_pred_test = np.concatenate(y_preds_test, axis=0)\n",
    "        y_test = np.concatenate(y_tests, axis=0)\n",
    "        \n",
    "        y_pred_train = np.concatenate(y_preds_train, axis=0)\n",
    "        y_train = np.concatenate(y_trains, axis=0)\n",
    "\n",
    "\n",
    "        if decision == \"argmax\": # for doing traditional decision making\n",
    "            f_scores_cell_seed_across_time_test = np.stack([f1_score(y_test[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            p_scores_cell_seed_across_time_test = np.stack([precision_score(y_test[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            r_scores_cell_seed_across_time_test = np.stack([recall_score(y_test[:,i], np.array([np.argmax(y_hat) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "\n",
    "        elif decision == 'threshmax':\n",
    "            tm = ThreshMax(classes=[0,1,2], thresh_class=1, class_to_optimize='avg')\n",
    "\n",
    "            thresholds = [tm.find_tmax(y_trues=y_train[:,i], y_preds=y_pred_train[:,i,:]) for i in range(y_train.shape[1])]\n",
    "            f_scores_cell_seed_across_time_test = np.stack([f1_score(y_test[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            p_scores_cell_seed_across_time_test = np.stack([precision_score(y_test[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            r_scores_cell_seed_across_time_test = np.stack([recall_score(y_test[:,i], np.array([tm.compute_threshmax(y_hat, thresholds[i]) for y_hat in y_pred_test[:,i,:]]), average=None) for i in range(y_test.shape[1])])\n",
    "            \n",
    "        f_scores_test[f'{real} {cell}'].append(f_scores_cell_seed_across_time_test)\n",
    "        p_scores_test[f'{real} {cell}'].append(p_scores_cell_seed_across_time_test)\n",
    "        r_scores_test[f'{real} {cell}'].append(r_scores_cell_seed_across_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=5\n",
    "f_scores_test = dict()\n",
    "p_scores_test = dict()\n",
    "r_scores_test = dict()\n",
    "# f_scores_train = dict()\n",
    "for cell in cell_list:\n",
    "    for real in ['Real', 'Random']:\n",
    "        f_scores_test[f'{real} {cell}'] = []\n",
    "        p_scores_test[f'{real} {cell}'] = []\n",
    "        r_scores_test[f'{real} {cell}'] = []\n",
    "        # f_scores_train[f'{real} {cell}'] = []\n",
    "# f_scores_train = {k: [] for k in cell_list} #gets score distribution for diff cells across splits\n",
    "for seed in range(seeds):\n",
    "    for gamma in range(5):\n",
    "        train_eval_RNNs(seed=seed, decision='threshmax', random_bl=False, sampling=None, multiclass_weights='sklearn', gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(f_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}\n",
    "p_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(p_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}\n",
    "r_scores_gamma = { f'gamma={gamma}': np.stack([item for index, item in enumerate(r_scores_test['Real LSTM']) if index % 5 == gamma], axis=0) for gamma in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(seeds):\n",
    "    train_eval_RNNs(seed=seed, decision='threshmax', random_bl=False, sampling=None, multiclass_weights='sklearn', gamma='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores_gamma['no gamma'] = np.stack(f_scores_test['Real LSTM'][-5:], axis=0)\n",
    "p_scores_gamma['no gamma'] = np.stack(p_scores_test['Real LSTM'][-5:], axis=0)\n",
    "r_scores_gamma['no gamma'] = np.stack(r_scores_test['Real LSTM'][-5:], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
